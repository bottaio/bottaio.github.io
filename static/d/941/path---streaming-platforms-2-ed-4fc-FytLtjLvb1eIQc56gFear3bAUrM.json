{"data":{"site":{"siteMetadata":{"title":"Adrian Bednarz's Blog","author":"Adrian Bednarz"}},"markdownRemark":{"id":"b84c2726-4101-5dd6-9507-95166ca6b0d0","excerpt":"Old fashioned batch architectures are not enough these days. With data comming from various sources - sockets, REST APIs, clickstream from…","html":"<p>Old fashioned batch architectures are not enough these days. With data comming from various sources - sockets, REST APIs, clickstream from the websites and logs it might no longer be feasible to just rely on batch jobs running every now and then - they might be just too slow. People tend to be moving towards more stream-oriented architectures.</p>\n<p>It’s not that we should get rid of the batch jobs altogether. ETL and other data warehouse feeding tasks are fields where these kind of jobs shine. We should no longer just save the data on ingestion in our systems. Apart from writing data to S3 or HDFS we would usually store it on a Kafka topic.</p>\n<p>Moreover, with Apache Kafka as a data distribution layer we can easily fix problems with some part of the system being temporarily unavailable. </p>\n<h2>Flink, Spark or…?</h2>\n<p>Let’s briefly walk through most famous Scala-oriented streaming platforms actively used in the industry.</p>\n<ol>\n<li>Apache Flink - fully stream architecture, deals with massive datasets with really low latency. Drawback: no batch support. They recently added SQL support.</li>\n<li>Apache Spark - most popular framework these days, formerly batch / mini-batch oriented - now with structured streaming it is supporting fully streaming jobs too. With great integrations: e. g. Tensorflow. It also supports SQL. </li>\n<li>Akka Streams - Akka allows for more low-level access to stream processing. Previously mentioned frameworks require programmer to build a job definition that later gets distributed across the cluster with little control. Akka Streams is a Scala library - you get more control and lower latency. But it is harder to optimize to process really massive datasets as Spark or Flink are capable of processing. Alpaka project can be used for integrations with other systems (databases, Kafka, queues etc.),</li>\n<li>Kafka Streams - similarly to Akka Streams its just a library. Sometimes we are just interested in transforming one Kafka topic to another - that is to read from some topics and to write to the others. This library is developed to help you with exactly that. It also supports SQL on topics.</li>\n</ol>\n<h2>Why SQL?</h2>\n<p>I mentioned SQL support - it is really important for analysts to have a way of defining transformation without Scala knowledge - as they might not necessarily know Scala well. An example of a project that addressed this problem back in 2010 is Apache Hive. There are other tools developed to help address this problem too - one of those is TouK Nussknacker - an Apache Flink GUI.</p>\n<h2>Everything still changes…</h2>\n<p>The project also worth mentioning is Apache Beam. It’s goal is to define data processing streams - apart from transformation it specifies how data delays should be addressed (commonly known as ingestion time vs event time). It was developed by Google and will probably be the next big thing (just as MapReduce became standard after all…).</p>\n<p>There are more technologies involved. Of course Zookeeper is required by the Kafka and it allows us to keep state around the cluster. Kubernetes or Mesos are next-generation cluster managers (that may soon deprecate YARN). With great power comes great responsibility - and a lot to learn.</p>","frontmatter":{"title":"[Big Data] Streaming platforms","date":"December 21, 2018"}}},"pageContext":{"slug":"/streaming-platforms/","previous":{"fields":{"slug":"/do-call/"},"frontmatter":{"title":"[R] Flatten list of data frames with do.call"}},"next":null}}